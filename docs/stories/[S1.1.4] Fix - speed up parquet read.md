STORY: Speed Up Parquet Reads 5–10× via Batched Symbol Loading

Objective

Reduce Fast Bench end-to-end runtime by 5–10× by eliminating per-symbol Parquet reads and replacing them with batched, vectorized Parquet scans.

This story focuses only on I/O orchestration.
No detection logic, scoring, sequencing, or evaluation behavior may change.

⸻

Non-Negotiable Constraints
	1.	Do NOT modify any detector logic
	•	Baseline and variants must behave identically
	2.	Do NOT modify baseline source files
	3.	Do NOT change output formats or filenames
	4.	Behavioral equivalence is mandatory
	•	Results must match bit-for-bit relative to the old harness

This is a performance-only refactor.

⸻

Current Problem

The harness currently:
	•	Reads Parquet once per symbol
	•	Executes serially
	•	Pays Arrow + filesystem overhead thousands of times

This is the dominant bottleneck.

⸻

High-Level Solution
	1.	Batch symbols in run.py
	2.	Read Parquet once per batch using IN filter pushdown
	3.	Split by symbol in memory
	4.	Feed symbol DataFrames to detectors exactly as before

This amortizes I/O overhead and unlocks vectorized Parquet performance.

⸻

Step 1 — Add batched Parquet read helper (io.py)

Add the following function to harness/io.py:

def read_symbol_batch(
    symbols: list[str],
    ohlcv_path: str,
    lookback_days: int = 0,
) -> pd.DataFrame:
    """
    Read OHLCV data for a batch of symbols in a single Parquet scan.
    """
    df = pd.read_parquet(
        ohlcv_path,
        filters=[("symbol", "in", symbols)],
    )

    if df.empty:
        return df

    df["date"] = pd.to_datetime(df["date"])

    if lookback_days > 0:
        cutoff = df["date"].max() - pd.Timedelta(days=lookback_days)
        df = df[df["date"] >= cutoff]

    return df.sort_values(["symbol", "date"]).reset_index(drop=True)

Do not remove or change the existing read_symbol_data() function.

⸻

Step 2 — Introduce batching in run.py

In harness/run.py:

Add a batch size parameter

Near config loading:

batch_size = int(cfg.get("read_batch_size", 200))


⸻

Replace the per-symbol loop

Before (current pattern):

for idx, symbol in enumerate(symbols, start=1):
    df = _io.read_symbol_data(symbol, ohlcv_path, lookback_days)
    ...

After (batched pattern):

from itertools import islice

def batched(iterable, size):
    it = iter(iterable)
    while True:
        batch = list(islice(it, size))
        if not batch:
            break
        yield batch


processed = 0

for symbol_batch in batched(symbols, batch_size):
    batch_df = _io.read_symbol_batch(
        symbol_batch,
        ohlcv_path,
        lookback_days,
    )

    if batch_df.empty:
        continue

    for symbol, df in batch_df.groupby("symbol"):
        processed += 1

        coverage_years += _io.compute_years_covered(df)

        for detector_name, detector_fn in detectors:
            events = detector_fn(df, cfg)
            if events.empty:
                continue

            events["detector"] = detector_name
            forward = add_forward_returns(events, df, forward_windows)
            forward["detector"] = detector_name

            events_buffers[detector_name].append(events)
            forward_buffers[detector_name].append(forward)

    if processed % flush_every == 0:
        flush_all_buffers()
        print(f"Processed {processed}/{len(symbols)} symbols")

All downstream logic remains unchanged.

⸻

Step 3 — Config addition (optional but recommended)

In harness/config.yaml, add:

read_batch_size: 200

Safe range:
	•	Laptop: 100–300
	•	Workstation: 300–800

⸻

Expected Performance Improvement

Change	Gain
Single Parquet scan per batch	3–5×
Arrow filter pushdown	2×
Reduced Python overhead	1.5–2×

Net result:
➡️ 5–10× faster full-universe runs
➡️ Zero algorithmic risk

⸻

Validation Checklist (MANDATORY)

After implementation:
	1.	Run old harness on a small symbol subset
	2.	Run new harness on the same subset
	3.	Confirm:
	•	Same event counts
	•	Same dates
	•	Same scores
	4.	Confirm:
	•	Output files identical (ordering may differ only by batch boundaries)

⸻

Explicitly Out of Scope
	•	Multiprocessing
	•	Threading
	•	Detector changes
	•	Sequencing logic
	•	Column pruning
	•	Caching

Those are future stories.

⸻

Acceptance Criteria

This story is complete when:
	•	Full-universe run time improves by ≥5×
	•	No detector logic was modified
	•	No baseline files were touched
	•	Outputs are behaviorally identical

⸻

End of Story

