STORY: Revert Batched Parquet Reads and Add Safe Symbol-Level Multiprocessing

CRITICAL SAFETY RULES (NON-NEGOTIABLE)
	1.	Do NOT modify baseline logic
	•	No changes to baseline/ files
	2.	Do NOT modify detector semantics
	•	Baseline and variants must emit identical events as before
	3.	Do NOT change output formats or filenames
	4.	Determinism is required
	•	Results must be identical to single-process runs (ordering may differ only in flush timing)

This is a performance-only refactor.

⸻

Objective
	1.	Revert the recent batched Parquet read change
(Return to optimal per-symbol reads, which match the current Hive-partitioned layout.)
	2.	Add safe multiprocessing at the symbol level
	•	Default to 8 workers
	•	Add a YAML config dial to control worker count

Target outcome: 5–7× faster full-universe runs on current hardware.

⸻

Context (Why This Is Correct)
	•	Parquet is already partitioned by symbol (symbol=AAA/…)
	•	Per-symbol reads are optimal
	•	CPU (detector logic) is the bottleneck
	•	Symbol-level tasks are independent → ideal for ProcessPoolExecutor

⸻

Step 1 — Revert Batched Parquet Reads

harness/io.py
	•	Keep read_symbol_data(...) unchanged.
	•	Remove or ignore read_symbol_batch(...) (do not use it anywhere).
	•	No other I/O changes.

harness/run.py
	•	Restore the per-symbol read path:

df = _io.read_symbol_data(symbol, ohlcv_path, lookback_days)


	•	Remove any batching helpers (batched(), read_batch_size, read_symbol_batch usage).

This returns us to the optimal I/O path for the current dataset layout.

⸻

Step 2 — Add YAML Dial for Worker Count

harness/config.yaml

Add:

workers: 8

Notes:
	•	Default = 8
	•	Valid range: 1–12 (caller responsibility)
	•	workers: 1 must behave exactly like the old single-process run

⸻

Step 3 — Introduce Safe Multiprocessing in run.py

Design constraints (MANDATORY)
	•	Parallelize by symbol, not by rows or batches
	•	No shared mutable state across workers
	•	Workers must not write files
	•	Parent process owns all I/O
	•	Exceptions in workers must fail fast

⸻

Add a worker function (top-level, picklable)

In harness/run.py, add:

def _process_symbol(
    symbol: str,
    ohlcv_path: str,
    lookback_days: int,
    cfg: dict,
    detectors,
):
    import pandas as pd
    from harness import io as _io
    from harness.eval import add_forward_returns

    df = _io.read_symbol_data(symbol, ohlcv_path, lookback_days)
    if df is None or df.empty:
        return symbol, [], []

    events_out = []
    forward_out = []

    for detector_name, detector_fn in detectors:
        events = detector_fn(df, cfg)
        if events.empty:
            continue

        events["detector"] = detector_name
        forward = add_forward_returns(events, df, cfg.get("forward_windows", [5, 10, 20, 40]))
        forward["detector"] = detector_name

        events_out.append(events)
        forward_out.append(forward)

    return symbol, events_out, forward_out

Important:
	•	This function returns data only
	•	No file writes
	•	No prints

⸻

Replace the main symbol loop with a process pool

In main() inside harness/run.py:
	1.	Read workers from config:

max_workers = int(cfg.get("workers", 8))

	2.	Replace the serial symbol loop with:

from concurrent.futures import ProcessPoolExecutor, as_completed

processed = 0

with ProcessPoolExecutor(max_workers=max_workers) as executor:
    futures = [
        executor.submit(
            _process_symbol,
            symbol,
            ohlcv_path,
            lookback_days,
            cfg,
            detectors,
        )
        for symbol in symbols
    ]

    for fut in as_completed(futures):
        symbol, events_list, forward_list = fut.result()

        for events in events_list:
            events_buffers[events["detector"].iloc[0]].append(events)

        for forward in forward_list:
            forward_buffers[forward["detector"].iloc[0]].append(forward)

        processed += 1

        if processed % flush_every == 0:
            for detector_name, _ in detectors:
                _flush_buffers(
                    events_buffers[detector_name],
                    forward_buffers[detector_name],
                    paths[detector_name]["events"],
                    paths[detector_name]["forward"],
                )
                events_buffers[detector_name].clear()
                forward_buffers[detector_name].clear()

            print(f"Processed {processed}/{len(symbols)} symbols")

	3.	Keep the final flush and summary logic unchanged.

⸻

Step 4 — Determinism & Safety Checks

After implementation:
	1.	Run with:

workers: 1

	•	Output must match pre-change runs exactly.

	2.	Run with:

workers: 8

	•	Event counts, dates, scores must match (ordering may differ).
	•	Runtime should drop ~5–7×.

⸻

Explicitly Out of Scope
	•	Threading
	•	Batched Parquet reads
	•	Detector changes
	•	Baseline changes
	•	GPU / numba / vectorization

⸻

Acceptance Criteria

This story is complete when:
	•	Batched Parquet reads are fully reverted
	•	Symbol-level multiprocessing is active
	•	Default worker count is 8
	•	Worker count is configurable via YAML
	•	Results are deterministic
	•	End-to-end runtime improves materially (≥5×)

⸻

End of Story
